{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnDIBA0s4VGv"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDNikAZ_420K"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, make_scorer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukYTH3Fy4a_o"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('drive/MyDrive/June 24-25/mtsamples.csv')\n",
    "data.loc[data.medical_specialty == ' Cardiovascular / Pulmonary', \"medical_specialty\"] = 'Heart'\n",
    "data.loc[data.medical_specialty == ' Neurosurgery', 'medical_specialty'] = 'Brain'\n",
    "data.loc[data.medical_specialty == ' Neurology', 'medical_specialty'] = 'Brain'\n",
    "data.loc[data.medical_specialty == ' Urology', 'medical_specialty'] = 'Reproductive'\n",
    "data.loc[data.medical_specialty == ' Obstetrics / Gynecology', 'medical_specialty'] = 'Reproductive'\n",
    "data.loc[data.medical_specialty == ' Gastroenterology', 'medical_specialty'] = 'Digestive'\n",
    "data.loc[data.medical_specialty == ' Nephrology', 'medical_specialty'] = 'Digestive'\n",
    "data = data[data.medical_specialty.isin(['Heart', 'Brain', 'Reproductive', 'Digestive'])]\n",
    "data['medical_specialty'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65KpMhDz4ki1"
   },
   "outputs": [],
   "source": [
    "data = data[['transcription', 'medical_specialty']]\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFQQs-3mFoUQ"
   },
   "outputs": [],
   "source": [
    "data['medical_specialty'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Snj65Mi-wdyz"
   },
   "outputs": [],
   "source": [
    "data.rename(columns = {'transcription':'Report', 'medical_specialty':'speciality'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLfXwTzuFpch"
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "  text = text.lower()\n",
    "  text = text.strip()\n",
    "  text = re.compile('[/(){}\\[\\]\\|@,;]').sub(' ', text) \n",
    "  text = re.compile('[^0-9a-z #+_]').sub('', text) \n",
    "  words = text.split()\n",
    "  i = 0 \n",
    "  while i < len(words):\n",
    "    if words[i] in stopwords.words('english'):\n",
    "      words.pop(i)\n",
    "    else:\n",
    "      i += 1\n",
    "    \n",
    "    return ' '.join(map(str, words))\n",
    "\n",
    "def lemmatize(text):\n",
    "    wordlist=[]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    sentences=sent_tokenize(text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words=word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            wordlist.append(lemmatizer.lemmatize(word))    \n",
    "    return ' '.join(wordlist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIYo4SDVFqBj"
   },
   "outputs": [],
   "source": [
    "data['transcription'] = data['transcription'].apply(clean)\n",
    "data['transcription'] = data['transcription'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xuPcsrCTISwZ"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1,3), max_df=0.75, use_idf=True, smooth_idf=True, max_features=1000)\n",
    "tfIdfMat  = vectorizer.fit_transform(data['Report'].tolist() )\n",
    "feature_names = sorted(vectorizer.get_feature_names())\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTQj49GlIfvZ"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "pca = PCA(n_components=0.95)\n",
    "tfIdfMat_reduced = pca.fit_transform(tfIdfMat.toarray())\n",
    "labels = data['speciality'].tolist()\n",
    "category_list = data.speciality.unique()\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfIdfMat_reduced, labels, stratify=labels, test_size=0.2, random_state=42)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xogZE7BeH2GF"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "y_test_pred= clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfQWuqMdJRiF"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_test_pred,labels=category_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cONaLr9tKWAT"
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c96YVxOSWKb"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "tfIdfMatrix = tfIdfMat.todense()\n",
    "labels = data['speciality'].tolist()\n",
    "tsne_results = TSNE(n_components=2,init='random',random_state=0, perplexity=40).fit_transform(tfIdfMatrix)\n",
    "plt.figure(figsize=(16,10))\n",
    "palette = sns.hls_palette(4, l=.6, s=.9)\n",
    "sns.scatterplot(\n",
    "    x=tsne_results[:,0], y=tsne_results[:,1],\n",
    "    hue=labels,\n",
    "    palette= palette,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTCAp0Y9vwnl"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, 'LR.pkl')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nonDL_models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
